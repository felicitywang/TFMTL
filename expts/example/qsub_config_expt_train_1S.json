{
    // this is for train mode only
    //////// meta-config ////////
    // Modify these!!!
    // Change these paths to yours
    "bashrc_path": "/home/fwang/.bashrc",
    "cpu_venv": "source /home/fwang/cpu/bin/activate",
    "gpu_venv": "source /home/fwang/gpu/bin/activate",
    "code_path": "/export/a08/fwang/tfmtl/expts/scripts/discriminative_driver.py",
    "email": "fwang40@jhu.edu",
    "username": "fwang",
    "cpu_or_gpu": "cpu",
    // main expts folder
    "root_dir": "/export/a08/fwang/tfmtl/expts/material",
    // "root_dir": ".",
    //////// meta-config ////////
    "debug": false,
    "mode": "train",
    // input key: default tokens
    // which domains to use
    "domains": [
        "GOV",
        "MIL",
        "BUS",
        "LAW",
        "REL"
    ],
    // experiment name
    "expt_setup_name": "",
    // dataset suffixes
    "dataset_suffixes": [
        "_gold_only"
    ],
    // what architecture in encoders.json to use
    "architecture": //
    // "dan_meanmax_relu_0.1_nopretrain",
    "dan_meanmax_relu_0.1_glove_only_finetune",
    // which args to use
    "args_paths": [
        // "min_1_max_-1_vocab_-1_doc_-1_tok_lower"
        "min_1_max_-1_vocab_-1_doc_-1_tok_lower_glove.6B.300d_only"
    ],
    // No need to modify
    // CLSP Grid related
    // for gpu: total_slots = gpu quota (=5)
    //          slots_per_job = 1
    // for cpu: total_slots = cpu quota (=100)
    //          slots_per_job = # cpu cores to use
    // whether to use cpu or gpu
    "gpu_total_slots": 5,
    "gpu_slots_per_job": 2,
    "cpu_total_slots": 200,
    "cpu_slots_per_job": 2,
    // mem_ram in GB
    "mem_ram": 3,
    "num_intra_threads": 1,
    "num_inter_threads": 1,
    // Hyper-parameters / default relative paths
    // datasets
    "datasets": "<domains>",
    // dataset_name
    "dataset_name": "<domains><dataset_suffixes>",
    // dataset paths: where the data.json.gz is stored
    "dataset_paths": "<root_dir>/data/tf/single/<dataset_name>/<args_paths>",
    // topics paths
    "topics_paths": "<root_dir>/data/json/<dataset_name>",
    // where to store results
    "results_dir": "<root_dir>/data/results/<expt_setup_name>/<name>",
    // where to store models(checkpoints)
    "checkpoint_dir": "<root_dir>/data/results/<expt_setup_name>/<name>/ckpt",
    // for finetune mode only
    // where to store summaries(for tensorboard), comment if not storing summaries
    // "summaries_dir": "<root_dir>/data/results/<expt_setup_name>/<name>/summ",
    // where to store logs
    "log_file": "<root_dir>/data/results/<expt_setup_name>/<name>/<mode>.log",
    // configuration of encoders.json, will be read to generate encoders.json for each dataset
    "encoder_config_file": "<root_dir>/data/results/<expt_setup_name>/<name>/encoders.json",
    // default to false in stl mode
    "embedders_tied": false,
    "extractors_tied": false,
    // Optional to modify
    "num_train_epochs": 200,
    // no early stopping when 1.0
    "early_stopping_acc_threshold": 0.0,
    // only needed when there's early stopping
    // "patience": 3,
    // default to 1 in STL mode
    "alphas": "1",
    // default to 2 for MATERIAL
    "class_sizes": "2",
    "model": "mult",
    "shared_mlp_layers": 0,
    "shared_hidden_dims": 0,
    // how many MLP layers and how dimensions of each
    "private_mlp_layers": 1,
    "private_hidden_dims": 128,
    "input_keep_prob": 1.0,
    "output_keep_prob": 1.0,
    "l2_weight": 0.0,
    "optimizer": "rmsprop",
    "lr0": 0.0001,
    // classification
    "metrics": "Acc Precision_Macro Recall_Macro F1_Macro",
    "tuning_metric": "Acc",
    // regression
    // "metrics": "Acc MSE",
    // "tuning_metric": "MSE",
    // "reporting_metric": "Acc MSE",
    "seed": [
        42
    ]
}
